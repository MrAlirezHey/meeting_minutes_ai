# -*- coding: utf-8 -*-
"""meeting_minutes.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xrPXKvKRQjNDtDqS0RZGTcdk2V2f_DoW
"""

!pip install -q --upgrade torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124
!pip install -q requests bitsandbytes transformers accelerate openai

!pip install librosa

from transformers import AutoTokenizer , AutoModelForCausalLM , TextStreamer ,BitsAndBytesConfig
from IPython.display import display,update_display , Markdown
import os
from openai import OpenAI
import torch

from transformers import AutoProcessor , AutoModelForSpeechSeq2Seq
from transformers import pipeline
import librosa

audio_array, sampling_rate = librosa.load('/content/denver_extract.mp3', sr=16000)

AUDIO_MODEL = "openai/whisper-medium"
model=AutoModelForSpeechSeq2Seq.from_pretrained(AUDIO_MODEL,device_map="auto",torch_dtype=torch.float16, low_cpu_mem_usage=True, use_safetensors=True)
processor=AutoProcessor.from_pretrained(AUDIO_MODEL)
pipe=pipeline(
    "automatic-speech-recognition",
    model=model,
    tokenizer=processor.tokenizer,
    feature_extractor=processor.feature_extractor,
    torch_dtype=torch.float16,
    return_timestamps=True

)

result=pipe(audio_array)

transcriptions=result['text']
print(transcriptions)

system_message = "You are an assistant that produces minutes of meetings from transcripts, with summary, key discussion points, takeaways and action items with owners, in markdown."
user_prompt = f"Below is an extract transcript of a Denver council meeting. Please write minutes in markdown, including a summary with attendees, location and date; discussion points; takeaways; and action items with owners.\n{transcriptions}"
messages=[
    {'role':'system','content':system_message},
    {'role':'user','content':user_prompt}

]

QWEN2 = "Qwen/Qwen2-7B-Instruct"

quat_config=BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_compute_dtype=torch.bfloat16,
    bnb_4bit_quant_type="nf4"
)

tokenizer=AutoTokenizer.from_pretrained(QWEN2)
tokenizer.pad_token=tokenizer.eos_token
model=AutoModelForCausalLM.from_pretrained(QWEN2,device_map='auto',quantization_config=quat_config,low_cpu_mem_usage=True)
inputs = tokenizer.apply_chat_template(messages, return_tensors="pt",add_generation_prompt=True).to("cuda")
streamer=TextStreamer(tokenizer)
output=model.generate(inputs, max_new_tokens=2000, streamer=streamer)
response = tokenizer.decode(output[0])

display(Markdown(response))

"""OPENAI"""

# @title
from google.colab import userdata
api_key=userdata.get('OPENAI_API_KEY')
openai=OpenAI(api_key=api_key)

"""# New Section"""

audio_file=open('/content/denver_extract.mp3','rb')
AUDIO_MODEL = "whisper-1"
transcript=openai.audio.transcriptions.create(model=AUDIO_MODEL,file=audio_file,respose_format='text')

def metting_minuts(audio):
  transcriptions=transcriptos(audio)
  system_message = "You are an assistant that produces minutes of meetings from transcripts, with summary, key discussion points, takeaways and action items with owners, in markdown."
  user_prompt = f"Below is an extract transcript of a Denver council meeting. Please write minutes in markdown, including a summary with attendees, location and date; discussion points; takeaways; and action items with owners.\n{transcriptions}"
  messages=[
    {'role':'system','content':system_message},
    {'role':'user','content':user_prompt}
    ]
  quat_config=BitsAndBytesConfig(
      load_in_4bit=True,
      bnb_4bit_use_double_quant=True,
      bnb_4bit_compute_dtype=torch.bfloat16,
      bnb_4bit_quant_type='nf4'
  )
  QWEN2 = "Qwen/Qwen2-7B-Instruct"
  tokenizer=AutoTokenizer.from_pretrained(QWEN2)
  inputs=tokenizer.apply_chat_template(messages,add_generation_prompt=True,return_tensors='pt').to('cuda')
  model=AutoModelForCausalLM.from_pretrained(QWEN2,device_map='auto',quantization_config=quat_config)
  streamer=TextStreamer(tokenizer)
  output=model.generate(**inputs,streamer=streamer,max_new_tokens=2000)
  return tokenizer.decode(output[0])

def transcriptos(audio_file):
  audio_array, sampling_rate = librosa.load(audio_file, sr=16000)
  AUDIO_MODEL = "openai/whisper-medium"
  Processor=AutoProcessor.from_pretrained(AUDIO_MODEL)
  model=AutoModelForSpeechSeq2Seq.from_pretrained(AUDIO_MODEL,device_map='auto',torch_dtype=torch.float16, low_cpu_mem_usage=True, use_safetensors=True)
  pipe=pipeline(
    "automatic-speech-recognition",
    model=model,
    tokenizer=Processor.tokenizer,
    feature_extractor=Processor.feature_extractor,
    torch_dtype=torch.float16,
    return_timestamps=True

  )
  result=pipe(audio_array)
  return result['text']

import gradio as gr
view=gr.Interface(fn=metting_minuts,
                  inputs=gr.Audio(sources='upload',
                                  type='filepath',
                                  label='upload your voice'),
                  outputs=gr.Markdown("Ai output"),
                  title='metting_mibuts',
                  description='upload your mp3 or wav file',
                  flagging_mode='never')

view.launch(share=False)

